<!DOCTYPE html>
<html>
<head>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-WLB3DPRW48"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-WLB3DPRW48');
  </script>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <meta content="NegVQA: Can Vision Language Models Understand Negation?" property="og:title">
  <meta content="NegVQA: Can Vision Language Models Understand Negation?" property="twitter:title">
  <meta content="We introduce NegVQA, a visual question answering benchmark designed to assess VLMs' comprehension of negation, revealing significant challenges in their understanding of this fundamental linguistic phenomenon." name="description">
  <meta content="We introduce NegVQA, a visual question answering benchmark designed to assess VLMs' comprehension of negation, revealing significant challenges in their understanding of this fundamental linguistic phenomenon." property="og:description">
  <meta content="We introduce NegVQA, a visual question answering benchmark designed to assess VLMs' comprehension of negation, revealing significant challenges in their understanding of this fundamental linguistic phenomenon." property="twitter:description">
  <meta property="og:type" content="website">
 
  <title>NegVQA: Can Vision Language Models Understand Negation?</title>
  <meta name="description" content="We introduce NegVQA, a visual question answering benchmark designed to assess VLMs' comprehension of negation.">
  <meta name="keywords" content="vision language models, negation, evaluation, visual question answering">
  <link rel="icon" type="image/x-icon" href="assets/favicon_io/favicon.png">
  <link rel="apple-touch-icon" sizes="180x180" href="assets/favicon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="assets/favicon_io/favicon.png">
  <link rel="icon" type="image/png" sizes="16x16" href="assets/favicon_io/favicon.png">

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="stylesheet" href="./static/css/custom.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>

  <style>
    .container {
      max-width: 1200px;
      margin: 0 auto;
      padding: 20px;
    }
    .badge {
      display: inline-block;
      margin: 2px;
    }
    .badge img {
      height: 20px;
    }
    pre {
      background-color: #f5f5f5;
      padding: 15px;
      border-radius: 5px;
      overflow-x: auto;
    }
    .links {
      margin: 20px 0;
    }
    .links a {
      display: inline-block;
      margin: 5px 10px 5px 0;
      padding: 8px 15px;
      background-color: #3273dc;
      color: white;
      text-decoration: none;
      border-radius: 4px;
    }
    .links a:hover {
      background-color: #2366d1;
    }
    img {
      max-width: 100%;
      height: auto;
      margin: 20px 0;
    }
  </style>

</head>
<body>

<div class="container">
  <h1>NegVQA: Can Vision Language Models Understand Negation?</h1>

  <div class="badge">
    <img src="https://img.shields.io/badge/License-MIT-blue.svg" alt="MIT license">
  </div>
  <div class="badge">
    <img src="https://img.shields.io/badge/python-3.11-blue.svg" alt="Python">
  </div>
  <div class="badge">
    <img src="https://img.shields.io/badge/Pytorch-2.5-red.svg" alt="Pytorch">
  </div>
  <div class="badge">
    <img src="https://img.shields.io/badge/code%20style-black-000000.svg" alt="Black">
  </div>

  <p>This repo provides the PyTorch source code of our paper: <a href="https://www.arxiv.org/abs/2505.22946">NegVQA: Can Vision Language Models Understand Negation?</a> (<strong>ACL 2025 Findings</strong>). Check out project page <a href="https://yuhui-zh15.github.io/NegVQA/">here</a>!</p>

  <div class="links">
    <a href="https://www.arxiv.org/abs/2505.22946" target="_blank">📄 Paper</a>
    <a href="https://github.com/yuhui-zh15/NegVQA" target="_blank">💻 Code</a>
    <a href="https://huggingface.co/datasets/yuhuizhang/NegVQA" target="_blank">🤗 Dataset</a>
    <a href="https://huggingface.co/datasets/yuhuizhang/NegVQA/tree/main/model_preds" target="_blank">📊 Model Predictions</a>
  </div>

  <h2>🔮 Abstract</h2>
  <p>Negation is a fundamental linguistic phenomenon that can entirely reverse the meaning of a sentence. As vision language models (VLMs) continue to advance and are deployed in high-stakes applications, assessing their ability to comprehend negation becomes essential. To address this, we introduce NegVQA, a visual question answering (VQA) benchmark consisting of 7,379 two-choice questions covering diverse negation scenarios and image-question distributions. We construct NegVQA by leveraging large language models to generate negated versions of questions from existing VQA datasets. Evaluating 20 state-of-the-art VLMs across seven model families, we find that these models struggle significantly with negation, exhibiting a substantial performance drop compared to their responses to the original questions. Furthermore, we uncover a U-shaped scaling trend, where increasing model size initially degrades performance on NegVQA before leading to improvements. Our benchmark reveals critical gaps in VLMs' negation understanding and offers insights into future VLM development. Project page available at .</p>

  <h2>🛠️ Adding Negation to VQA Datasets</h2>
  <p>Check out <a href="add_negation.ipynb">add_negation.ipynb</a> for the implementation of adding negation to VQA datasets.</p>

  <h2>💎 Dataset: NegVQA</h2>
  <img src="images/data.png" alt="NegVQA Dataset">
  <p>Dataset is available at <a href="https://huggingface.co/datasets/yuhuizhang/NegVQA">Huggingface</a>.</p>

  <h2>📈 Evaluation of NegVQA</h2>
  <img src="images/result.png" alt="NegVQA Results">
  <p>Model predictions are available at <a href="https://huggingface.co/datasets/yuhuizhang/NegVQA/tree/main/model_preds">Huggingface</a>. These predictions are generated using <a href="https://github.com/open-compass/VLMEvalKit">VLMEvalKit</a>. The running script is <a href="evaluate_models.sh">evaluate_models.sh</a>. Results are plotted from <a href="plot_results.ipynb">plot_results.ipynb</a>.</p>

  <h2>🎯 Citation</h2>
  <p>If you use this repo in your research, please cite it as follows:</p>
  <pre><code>@inproceedings{NegVQA,
  title={NegVQA: Can Vision Language Models Understand Negation?},
  author={Yuhui Zhang and Yuchang Su and Yiming Liu and Serena Yeung-Levy},
  booktitle={ACL 2025 (Findings)},
  year={2025}
}</code></pre>

</div>

</body>
</html>